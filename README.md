# NLP using DRL
Various of NLP tasks using Deep Reinforcement Learning

1. <a href="https://arxiv.org/abs/1609.08667">Deep Reinforcement Learning for Mention-Ranking Coreference Models</a>
2. <a href="https://arxiv.org/abs/1606.01541">Deep Reinforcement Learning for Dialogue Generation</a>
3. <a href="https://arxiv.org/abs/1705.04304">A Deep Reinforced Model for Abstractive Summarization</a>
4. <a href="https://arxiv.org/abs/1612.04936">Learning through Dialogue Interactions by Asking Questions</a>
5. <a href="https://arxiv.org/abs/1706.05125">Deal or No Deal? End-to-End Learning for Negotiation Dialogues</a>
6. <a href="https://arxiv.org/abs/1706.00130">Teaching Machines to Describe Images via Natural Language Feedback</a>
7. <a href="https://arxiv.org/abs/1704.04572">Task-Oriented Query Reformulation with Reinforcement Learning</a>
8. <a href="https://arxiv.org/abs/1707.06690">DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning</a>
9. <a href="https://arxiv.org/abs/1609.00777">Towards End-to-End Reinforcement Learning of Dialogue Agents for Information Access</a>


# Dialogue Policy Optimization

1. <a href="https://arxiv.org/pdf/1703.01008.pdf">End-to-End Task-Completion Neural Dialogue Systems</a>
2. <a href="http://emnlp2017.net/accepted-papers.html">Composite Task-Completion Dialogue Policy Learning via Hierarchical Deep Reinforcement Learning</a>
3. <a href="https://arxiv.org/pdf/1801.07243.pdf">PERSONALIZING DIALOGUE AGENTS:I HAVE A DOG, DO YOU HAVE PETS TOO?</a>

# Representation Learning for NLP

1. <a href="https://nlp.stanford.edu/pubs/glove.pdf">GloVe: Global Vectors for Word Representation</a>
2. <a href="https://arxiv.org/pdf/1506.06726.pdf">Skip-Thought Vectors</a>
3. <a href="https://arxiv.org/abs/1705.02364">Supervised Learning of Universal Sentence Representations from Natural Language Inference Data</a>
4. <a href="https://aclweb.org/anthology/W/W16/W16-1612.pdf">Mapping Unseen Words to Task-Trained Embedding Spaces</a>
5. <a href="https://arxiv.org/abs/1703.03130">A Structured Self-attentive Sentence Embedding</a>
6. <a href="https://arxiv.org/abs/1607.01759">Bag of Tricks for Efficient Text Classification</a>
7. <a href="https://arxiv.org/abs/1607.04606">Enriching Word Vectors with Subword Information</a>
8. <a href="https://arxiv.org/abs/1705.10359">Neural Embeddings of Graphs in Hyperbolic Space</a>
9. <a href="https://arxiv.org/abs/1705.08039">Poincaré Embeddings for Learning Hierarchical Representations</a>
10. <a href="https://einstein.ai/static/images/layouts/research/cove/McCann2017LearnedIT.pdf">Learned in Translation: Contextualized Word Vectors</a>
11. <a href="https://arxiv.org/abs/1309.4168">Exploiting Similarities among Languages for Machine Translation</a>
12. <a href="https://arxiv.org/abs/1707.09457">Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints</a>
13. <a href="https://arxiv.org/pdf/1605.02019.pdf">Mixing Dirichlet Topic Models and Word Embeddings to Make lda2vec</a>
14. <a href="https://arxiv.org/pdf/1803.02893.pdf">AN EFFICIENT FRAMEWORK FOR LEARNING SENTENCE REPRESENTATIONS</a>


# NLP

1. <a href="https://arxiv.org/abs/1708.02182">Regularizing and Optimizing LSTM Language Models</a>
2. <a href="https://arxiv.org/abs/1607.06450">Layer Normalization</a>
3. <a href="https://openreview.net/pdf?id=Byj72udxe">POINTER SENTINEL MIXTURE MODELS</a>
4. <a href="https://arxiv.org/abs/1709.02755">Training RNNs as Fast as CNNs</a>
5. <a href="https://arxiv.org/pdf/1702.01806.pdf">Beam Search Strategies for Neural Machine Translation</a>
6. <a href="https://nlp.stanford.edu/pubs/zhang2017tacred.pdf">Position-aware Attention and Supervised Data Improve Slot Filling</a>
7. <a href="https://arxiv.org/pdf/1603.08148.pdf">Pointing the Unknown Words</a>
8. <a href="https://arxiv.org/abs/1710.02224">Dilated Recurrent Neural Networks</a>
9. <a href="https://arxiv.org/pdf/1711.00066.pdf">FRATERNAL DROPOUT</a>
10. <a href="https://arxiv.org/pdf/1711.03953.pdf">BREAKING THE SOFTMAX BOTTLENECK:A HIGH-RANK RNN LANGUAGE MODEL</a>
11. <a href="https://arxiv.org/pdf/1711.05717.pdf">VARIATIONAL BI-LSTMS</a>
12. <a href="https://www.csie.ntu.edu.tw/~yvchen/doc/ICASSP17_E2E.pdf">END-TO-END JOINT LEARNING OF NATURAL LANGUAGE UNDERSTANDING AND DIALOGUE MANAGER</a>
13. <a href="http://papers.nips.cc/paper/7130-svd-softmax-fast-softmax-approximation-on-large-vocabulary-neural-networks.pdf">SVD-Softmax: Fast Softmax Approximation on Large Vocabulary Neural Networks</a>
14. <a href="https://arxiv.org/pdf/1706.08502.pdf">Natural Language Does Not Emerge ‘Naturally’ in Multi-Agent Dialog</a>
15. <a href="https://arxiv.org/pdf/1704.01444.pdf">Learning to Generate Reviews and Discovering Sentiment</a>
16. <a href="https://arxiv.org/pdf/1711.06788v1.pdf">MinimalRNN: Toward More Interpretable and Trainable Recurrent Neural Networks</a>
17. <a href="https://arxiv.org/pdf/1606.02960.pdf">Sequence-to-Sequence Learning as Beam-Search Optimization</a>

# Machine Reading Comprehension

1. <a href="https://arxiv.org/abs/1611.01603">Bidirectional Attention Flow for Machine Comprehension</a>
2. <a href="https://arxiv.org/abs/1711.00106">DCN+: Mixed Objective and Deep Residual Coattention for Question Answering</a>
3. <a href="https://openreview.net/pdf?id=BJIgi_eCZ">FUSIONNET: FUSING VIA FULLY-AWARE ATTENTION WITH APPLICATION TO MACHINE COMPREHENSION</a>
4. <a href="https://openreview.net/pdf?id=B14TlG-RW">Fast and Accurate Reading Comprehension by Combining Self-Attention and Convolution</a>


# Speech Recognition

1. <a href="https://arxiv.org/abs/1508.01211">Listen, Attend and Spell</a>
2. <a href="https://arxiv.org/abs/1512.02595">Deep Speech 2: End-to-End Speech Recognition in English and Mandarin</a>
3. <a href="https://arxiv.org/abs/1712.01769">State-of-the-art Speech Recognition With Sequence-to-Sequence Models</a>

# Neural Machine Translation

1. <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>
2. <a href="https://arxiv.org/abs/1711.00043">Unsupervised Machine Translation Using Monolingual Corpora Only</a>
3. <a href="https://arxiv.org/abs/1603.06393">Incorporating Copying Mechanism in Sequence-to-Sequence Learning</a>
4. <a href="https://arxiv.org/pdf/1804.00247.pdf">Training Tips for the Transformer Model</a>

# Reinforcement Learning 

1. <a href="https://arxiv.org/abs/1707.08616">Guiding Reinforcement Learning Exploration Using Natural Language</a>
2. <a href="https://arxiv.org/abs/1704.03732">Learning from Demonstrations for Real World Reinforcement Learning</a>
3. <a href="https://arxiv.org/abs/1707.06887">A Distributional Perspective on Reinforcement Learning</a>
4. <a href="https://arxiv.org/abs/1703.03864">Evolution Strategies as a Scalable Alternative to Reinforcement Learning</a>
5. <a href="https://arxiv.org/abs/1706.01905">Parameter Space Noise for Exploration</a>
6. <a href="https://arxiv.org/abs/1709.04326">Learning with Opponent-Learning Awareness</a>
7. <a href="https://arxiv.org/abs/1705.05363">Curiosity-driven Exploration by Self-supervised Prediction</a>
8. <a href="https://arxiv.org/abs/1710.09767.pdf">META LEARNING SHARED HIERARCHIES</a>
9. <a href="https://arxiv.org/abs/1707.06347">Proximal Policy Optimization Algorithms</a>
10. <a href="https://arxiv.org/pdf/1611.01144.pdf">CATEGORICAL REPARAMETERIZATION WITH GUMBEL-SOFTMAX</a>
11. <a href="https://arxiv.org/pdf/1710.02298.pdf">Rainbow: Combining Improvements in Deep Reinforcement Learning</a>
